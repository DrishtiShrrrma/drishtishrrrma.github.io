---
layout: post
title: "Multilingual Long Chain-of-Thoughts with Small Reasoning Models"
date: 2025-05-15 00:00:00 +0000
image: /images/multilingual-long-cot.png  # optional
categories: other
rank: 6
author: "Drishti Sharma"
authors: "Swati Rajwal, Shayekh Islam, Marek Suppa, <strong>Drishti Sharma</strong>, Azmine Toushik, Yadnyesh C., Morteza Kashani, Allison Yang, Ira Salsabila"
code: https://github.com/Multilingual-long-COT
slides: https://docs.google.com/presentation/d/1wN7g0vDMFHGgERIk2jJc1xF1pGywjRsijZmyryoNWqI/edit?slide=id.g356660ebed0_0_74#slide=id.g356660ebed0_0_74
link: https://docs.google.com/presentation/d/1wN7g0vDMFHGgERIk2jJc1xF1pGywjRsijZmyryoNWqI/edit?slide=id.g356660ebed0_0_74#slide=id.g356660ebed0_0_74
hf org: https://huggingface.co/mutilingual-long-cot
excerpt: "This project investigates the performance of small reasoning models on long-form multilingual chain-of-thought (CoT) tasks in scientific domains. Using translated and human-post-edited versions of GPQA, we evaluate models like Qwen3-1.7B and Deepseek-R1-Distill across six languages including Hindi, Bengali, Spanish, and Marathi. Our findings reveal that small models underperform in low-resource languages and generate significantly shorter CoT traces. By fine-tuning models on multilingual reasoning traces derived from CAMEL-AI, we aim to democratize access to high-quality scientific reasoning in non-English contexts."
---
