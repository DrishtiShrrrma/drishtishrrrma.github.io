---
layout: post
title: "DistAYA"
categories: other
date: 2024-08-11 00:00:00 +0000
image: /images/distaya.png  # optional — you can crop and save the image
author: Drishti Sharma
authors: "Guijin Son, Yaya, Mayank Bhaskar, Shayekh, Ahmad Anis, Vishnu Lanka, Roshan Santhosh, <strong>Drishti Sharma</strong>"
slides: https://docs.google.com/presentation/d/1qIMBGS3Lz1oA4XRddkOHikQu4kWPvQ2xoogCNlu22u8/edit?slide=id.g2833461dc11_3_5
hf org: https://huggingface.co/DistAya
excerpt: "DistAYA addresses the practical challenges of deploying large multilingual language models by focusing on model compression techniques—such as pruning, quantization, and distillation—that aim to improve efficiency without sacrificing performance or linguistic coverage. Centered on the Aya23-8B model, the project systematically explores a range of methods. Among them, SparseGPT, an unstructured and semi-structured pruning technique, proves especially effective—achieving up to 50% sparsity without requiring retraining, using a layer-wise reconstruction approach. The project also evaluates ShortGPT, a layer-pruning method that removes components with minimal impact on output, revealing redundancy in large models. While other strategies like quantization and task-aware distillation are explored, the strongest results come from pruning-based approaches."
---


**DistAYA** tackles real-world deployment challenges of multilingual LLMs by compressing Aya23-8B without sacrificing performance.  

The team explored and compared multiple efficient inference techniques, including:

- **SparseGPT**: Enables up to 50% pruning with no retraining using a one-shot reconstruction algorithm.
- **ShortGPT**: Prunes the least important layers based on influence metrics like KLD.
- **DistillKit**: Uses hidden state distillation to train a compact student model.
- **Qwen2-1.5B fine-tuning**: Targeted model selection and fine-tuning.

The goal was to retain Aya23-8B’s **linguistic diversity** while drastically reducing **latency**, **memory**, and **model size** — making it ready for **production-scale multilingual applications**.

View code and models on [HuggingFace](https://huggingface.co/DistAya).
