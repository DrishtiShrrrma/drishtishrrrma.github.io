---
title: "DistAYA"
excerpt: "Efficient multilingual LLM deployment using pruning, quantization, and distillation techniques on Aya23-8B."
image: images/distaya-thumb.png  # optional — you can crop and save the image you just uploaded
categories: other
date: 2024-08-11
slides: https://docs.google.com/presentation/d/1qIMBGS3Lz1oA4XRddkOHikQu4kWPvQ2xoogCNlu22u8/edit?slide=id.g2833461dc11_3_5
website: https://huggingface.co/DistAya
authors: "Guijin Son, Yaya, Mayank Bhaskar, Shayekh, Ahmad Anis, Vishnu Lanka, Roshan Santhosh, Drishti Sharma"
---

**DistAYA** tackles real-world deployment challenges of multilingual LLMs by compressing Aya23-8B without sacrificing performance.  

The team explored and compared multiple efficient inference techniques, including:

- **SparseGPT**: Enables up to 50% pruning with no retraining using a one-shot reconstruction algorithm.
- **ShortGPT**: Prunes the least important layers based on influence metrics like KLD.
- **DistillKit**: Uses hidden state distillation to train a compact student model.
- **Qwen2-1.5B fine-tuning**: Targeted model selection and fine-tuning.

The goal was to retain Aya23-8B’s **linguistic diversity** while drastically reducing **latency**, **memory**, and **model size** — making it ready for **production-scale multilingual applications**.

View code and models on [HuggingFace](https://huggingface.co/DistAya).
