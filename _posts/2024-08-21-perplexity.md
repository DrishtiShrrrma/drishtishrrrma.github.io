---
layout: post
title: "Perplexity vs Evaluation: A Quantitative Study"
date: 2024-08-21 00:00:00 +0000
image: /images/perplexity.png
categories: other
author: Drishti Sharma
authors: "Mohammad Aflah Khan, Sharad Duwal, Roshan Santhosh, Girish, Shayekh, <strong>Drishti Sharma</strong>, Henok, Kamya, Nandini, Harshita, Timothy"
link: https://huggingface.co/perplexity-v-model-perf-eval
code: https://github.com/aflah02/Perplexity-vs-Evaluation
slides: https://docs.google.com/presentation/d/1mnb6YoujgCjeysoL1-0LvJfGpHguMWgG5cynwOXdjq4
excerpt: "This project investigates whether lower perplexity translates into better real-world model performance across tasks like summarization, instruction following, and multilingual QA. We benchmarked over 30 open-source LLMs and found that while perplexity does correlate with performance, the strength of that relationship depends heavily on language. Some models with low perplexity underperform on downstream tasks, especially in underrepresented languages. The results suggest that intrinsic linguistic characteristics and exposure during pretraining affect the connection between perplexity and quality â€” prompting a call for evaluation beyond perplexity alone."
---
